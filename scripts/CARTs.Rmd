---
title: "script_taller2"
output: null
date: "2025-03-19"
editor_options: 
  markdown: 
    wrap: 72
---

Cargando librerias

```{r}
require(pacman)

p_load(tidyverse, # tidy-data
       rpart, # Recursive Partition and Regression Trees (To run Trees)
       caret ,  # for model training and tunning
       rpart.plot, ## for trees graphs
       Metrics, ## Evaluation Metrics for ML
       MLmetrics, # Evaluation Metrics for ML
       ipred,  # For Bagging 
       ranger #For random Forest
       )   
```

Cargando los datos

```{r}
#Cargamos bases de datos 
raw_data_path <- '../data'
predictions_path <- '../results/predictions'

# Datos 
bd_train <- read.csv(file.path(raw_data_path, 'bd_train_limpia.csv'))
bd_test <- read.csv(file.path(raw_data_path, 'bd_test_limpia.csv'))

```

Haciendo ajustes sobre la variable predicha

```{r}
# Ajustando los niveles de la variable Pobre
bd_train <- bd_train %>% mutate(Pobre=factor(Pobre,levels=c(1,0),labels=c("pobre","No_pobre")))

# Verificando la estructura de la variable predicha. 
str(bd_train$Pobre)  
prop.table(table(bd_train$Pobre))
```

# CARTs

## Modelo básico

creando una función personalizada que use el F1 score para elegir el
mejor modelo.

```{r function_metrics}

sixStats <- function(data, lev = NULL, model = NULL) {
  # Calcula las métricas estándar
  twoClass <- twoClassSummary(data, lev, model)
  default <- defaultSummary(data, lev, model)
  
  # Calcula el F1 Score
  f1 <- F1_Score(data$obs, data$pred, positive = lev[1])
  
  # Combina todas las métricas
  c(twoClass, default, F1 = f1)
}


```


```{r}
# Control de entrenamiento
ctrl_c1 <- trainControl(
  method = "cv",
  number = 5,
  summaryFunction = sixStats,
  classProbs = TRUE,      # Necesario para ROC/F1
  savePredictions = TRUE, # Para calcular métricas
  verboseIter = FALSE
)

# especificamos la grilla de los alphas
grid <- expand.grid(cp = seq(0, 0.04, by = 0.01))
```

### Entrenando el modelo

```{r}
# Declarando la semilla para reproducibilidad
set.seed(123)  

# Entrenamiento del modelo
cv_tree_1 <- train(
    Pobre ~ Cabecera + Dominio + num_room + 
                      num_bed + propiedad + pago_amort + renta_h + 
                      renta_r + Nper + Depto + suma_antiguedad + promedio_antiguedad + 
                      tiene_empleado_publico + tiene_patron + tiene_cuenta_propia + 
                      tiene_emp_domestico + tiene_jornalero + tiene_sin_remuneracion+ 
                      n_posiciones_lab_distintas + aux_trans + ind_prima + prima_serv + prima_nav +
                      prima_vac + ind_viaticos + ind_oficio + ind_arriendo + pet_trabajo +
                      max_educ:ocupado + hr_extr + otro_tr + rem_ext + reg_cotiz + cotiz_pen + ing_otros +
                      edad_prom + perc_fem,
               data = bd_train,
               method = "rpart", 
               trControl = ctrl_c1, 
               tuneGrid = grid, 
               metric= "F1" 
               )
cv_tree_1

```

### Exportando predicciones

Ahora preparamos el archivo de output para subir la predicción de
pobreza a Kaggle usando la base de test.

```{r}
# Obtener predicciones (clases: "No_Pobre" o "Pobre")
predicciones_clase <- predict(cv_tree_1, newdata = bd_test)

# Convertir a 0 y 1 (para el formato de Kaggle)
predicciones_numericas <- ifelse(predicciones_clase == "No_pobre", 0, 1)

#Crear CSV para Kaggle
submission <- data.frame(
  id = bd_test$id,
  pobre = predicciones_numericas
)

# Create descriptive filename
filename <- sprintf("%s/CART-cp0_5fold_F1optimized_%s.csv", 
                   predictions_path,
                   format(Sys.time(), "%Y%m%d_%H%M"))

# 6. Save with proper format
write.csv(submission, 
          file = filename,
          row.names = FALSE,
          quote = FALSE)

```



## Modelo con ciertas variables

```{r}
# Declarando la semilla para reproducibilidad
set.seed(123)  

# Entrenamiento del modelo
cv_tree_2 <- train(
    Pobre ~ max_educ:ocupado + num_room + Nper +tiene_empleado_publico + tiene_patron + tiene_cuenta_propia +
      ind_prima + prima_serv + prima_nav + prima_vac + ind_viaticos + ocupado + cotiz_pen + edad_prom,
               data = bd_train,
               method = "rpart", 
               trControl = ctrl, 
               tuneGrid = grid, 
               metric= "F1" 
               )
cv_tree_2

```

##Modelo usando oversampling - best

```{r}
ctrl_cart <- trainControl(
  method = "cv",
  number = 5,
  sampling = "up",  # Upsampling de la clase "pobre" (oversampling)
  summaryFunction = sixStats,
  classProbs = TRUE
)
```


```{r}
grid_cart <- data.frame(cp = seq(0, 0.1, by = 0.02))  # Solo cp

# grid <- expand.grid(
#   cp = seq(0.0005, 0.01, by = 0.0005),  # Valores más bajos para cp
#   minsplit = c(10, 20, 30),              # Evita splits con muy pocos datos
#   minbucket = c(5, 10, 15)               # Controla el tamaño mínimo en hojas
# )

```

```{r}
cv_tree <- train(
      Pobre ~ Clase + Dominio + num_room + 
                      num_bed + propiedad + pago_amort + renta_h + 
                      renta_r + Nper + Depto + suma_antiguedad + promedio_antiguedad + 
                      tiene_empleado_publico + tiene_patron + tiene_cuenta_propia + 
                      tiene_emp_domestico + tiene_jornalero + tiene_sin_remuneracion+ 
                      n_posiciones_lab_distintas + aux_trans + ind_prima + prima_serv + prima_nav +
                      prima_vac + ind_viaticos + ocupado + ind_oficio + ind_arriendo + pet_trabajo +
                      max_educ + edad_prom + hr_extr + otro_tr + rem_ext + reg_cotiz + cotiz_pen + ing_otros +
                      perc_fem,  # Usa todas las variables (o ajusta la fórmula)
  data = bd_train,
  method = "rpart",
  trControl = ctrl_cart,
  tuneGrid = grid_cart,
  metric = "F1" 
)

cv_tree
```









































# Xgboost

Preparación de la base. Todas las variables deben ser numéricas

```{r}
#Selección de variables relevantes
variables_modelo <- c("Pobre", "Clase", "Dominio", "num_room", "num_bed", "propiedad",
                      "pago_amort", "renta_h", "renta_r", "Nper", "Depto", "suma_antiguedad", 
                      "promedio_antiguedad", "tiene_empleado_publico",
                      "tiene_patron", "tiene_cuenta_propia", "tiene_emp_domestico",       
                      "tiene_jornalero", "tiene_sin_remuneracion", 
                      "n_posiciones_lab_distintas", "aux_trans", "ind_prima",
                      "prima_serv", "prima_nav", "prima_vac", "ind_viaticos", 
                      "ocupado", "ind_oficio", "ind_arriendo", "pet_trabajo",                
                      "max_educ", "hr_extr", "otro_tr", "rem_ext", "reg_cotiz",
                      "cotiz_pen", "ing_otros", "edad_prom", "perc_fem")

#Preparación de la base de entrenamiento
setDT(bd_train)
train <- bd_train[, .SD, .SDcols = c("id", variables_modelo)]

#Preparación del conjunto de prueba 
variables_modelo_te <- setdiff(variables_modelo, "Pobre")
setDT(bd_test)
test <- bd_test[, .SD, .SDcols = c("id", variables_modelo_te)]

# Retirando NAs
train <- na.omit(train)

# Convirtiendo train en matrices de variables todas numéricas. 
train_numeric <- train[, lapply(.SD, function(x) as.numeric(as.character(x))), .SDcols = variables_modelo[-1]]
train_matrix <- xgb.DMatrix(
  data = as.matrix(train_numeric),
  label = as.numeric(train$Pobre) - 1
)


```


control

```{r}
# Configurar trainControl para F1
ctrl_boost <- trainControl(
  method = "cv",
  number = 5,
  summaryFunction = prSummary,  # Necesario para F1
  classProbs = TRUE,           # Para obtener probabilidades
  allowParallel = TRUE,
  verboseIter = TRUE, 
)

```

tunegrid:

```{r}
grid_boost <- expand.grid(
  nrounds = c(100, 150),           # Iteraciones
  max_depth = c(4, 6),             # Profundidad del árbol
  eta = c(0.01, 0.1),              # Tasa de aprendizaje
  gamma = c(0, 0.1),               # Reducción mínima de pérdida
  colsample_bytree = c(0.7, 0.9),  # Variables por árbol
  min_child_weight = c(1, 3),      # Peso mínimo en nodos
  subsample = c(0.7, 0.9)          # Muestreo de datos
)
```


```{r}
# Entrenar modelo XGBoost
boost_model <- xgboost(
  data = train_matrix,
  max.depth = 6,
  nrounds = 100,
  objective = "binary:logistic",
  verbose = 0, 
  params = list(scale_pos_weight = ratio)
)
```


```{r}
xgb_model <- train(
  x = train[, variables_modelo[-1], with = FALSE],
  y = train$Pobre,
  method = "xgbTree",
  trControl = ctrl_boost,
  tuneGrid = grid_boost,
  metric = "F",            # F1-Score (se llama "F" en prSummary)
  verbose = FALSE
)

# Resultados
print(xgb_model)
#plot(xgb_model)  # Gráfica de F1 vs. combinaciones de hiperparámetros
```


```{r}
model <- train(
        Pobre ~ Clase + Dominio + num_room + 
                      num_bed + propiedad + pago_amort + renta_h + 
                      renta_r + Nper + Depto + suma_antiguedad + promedio_antiguedad + 
                      tiene_empleado_publico + tiene_patron + tiene_cuenta_propia + 
                      tiene_emp_domestico + tiene_jornalero + tiene_sin_remuneracion+ 
                      n_posiciones_lab_distintas + aux_trans + ind_prima + prima_serv + prima_nav +
                      prima_vac + ind_viaticos + ocupado + ind_oficio + ind_arriendo + pet_trabajo +
                      max_educ + hr_extr + otro_tr + rem_ext + reg_cotiz + cotiz_pen + ing_otros +
                      edad_prom + perc_fem,
  data = bd_train,
  method = "xgbTree",
  trControl = ctrl_cart,
  tuneGrid = expand.grid(
    nrounds = 100,
    max_depth = c(3, 6),
    eta = 0.1,
    gamma = 0,
    colsample_bytree = 0.8,
    min_child_weight = 1,
    subsample = 0.8
  ),
  metric = "Sens"
)

model
```

#RF

```{r}
library(ranger)
model <- train(
  Pobre ~ .,
  data = datos,
  method = "ranger",
  trControl = ctrl,
  tuneGrid = data.frame(
    mtry = c(5, 10, 15),
    splitrule = "gini",
    min.node.size = c(1, 5, 10)
  ),
  metric = "Sens",
  class.weights = c("pobre" = 5, "No_pobre" = 1)  # Penaliza más los errores en "pobre"
)
```


